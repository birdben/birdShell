input {
    kafka {
        # 指定Zookeeper集群地址
        zk_connect => "hadoop1:2181,hadoop2:2181,hadoop3:2181"
        # 指定当前消费者的group_id
        group_id => "logstash"
        # 指定消费的Topic
        topic_id => "logstash_test"
        # 指定消费的内容类型（默认是json）
        codec => "json"
        # 设置Consumer消费者从Kafka最开始的消息开始消费，必须结合"auto_offset_reset => smallest"一起使用
        reset_beginning => true
        # 设置如果Consumer消费者还没有创建offset或者offset非法，从最开始的消息开始消费还是从最新的消息开始消费
        auto_offset_reset => "smallest"
        type => "api"
    }
}

filter {
    # 将logs数组对象进行拆分
    split {
        field => "logs"
    }
    date {
        match => ["timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
        target => "@timestamp"
    }
}

output {
    stdout {
        codec => rubydebug
    }
    elasticsearch {
        codec => "json"
        hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
        index => "api_logs_index"
        document_type => "%{type}"
        workers => 1
        flush_size => 20000
        idle_flush_time => 10
    }
}

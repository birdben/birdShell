input {
    kafka {
        # 指定Zookeeper集群地址
        zk_connect => "hadoop1:2181,hadoop2:2181,hadoop3:2181"
        # 指定当前消费者的group_id，group_id不能和其他logstash消费者相同，否则同时启动多个Logstash消费者offset会被覆盖
        group_id => "logstash_node"
        # 指定消费的Topic
        topic_id => "node_log"
        # 指定消费的内容类型（默认是json）
        codec => "json"
        # 设置Consumer消费者从Kafka最开始的消息开始消费，必须结合"auto_offset_reset => smallest"一起使用
        reset_beginning => true
        # 设置如果Consumer消费者还没有创建offset或者offset非法，从最开始的消息开始消费还是从最新的消息开始消费
        auto_offset_reset => "smallest"
    }
}

filter {
    if [type] == "node_track" {
        grok {
            match => {
                "message" => "%{MATCH_ALL:@message}"
            }
        }
        json {
            source => "@message"
        }
        if "_jsonparsefailure" in [tags] {
            drop {}
        } else {
            if [logs] {
                # 将logs数组对象进行拆分
                split {
                    field => "logs"
                }
                date {
                    match => ["timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
                    target => "@timestamp"
                    # 设置使用UTC标准时区，因为node的时间戳是UTC标准时区，不是中国时区，我们可以使用指定timezone来转换成long类型的@timestamp
                    locale => "en"
                    timezone => "+00:00"
                }
            } else {
                drop {}
            }
        }
    }
    if [type] == "node_access" {
        grok {
            match => {
                "message" => "%{NODE_ACCESS_LOGS}"
            }
        }
        if "_grokparsefailure" in [tags] {
            drop { }
        }
# 这里是geoip指定字段的用法，不适用于geoip2
        geoip {
            source => "client_ip"
            target => "geoip"
            # 这里指定好解压后GeoIP数据库文件的位置
            database => "/usr/local/logstash/geoip/GeoLiteCity.dat"
            # 直接使用location字段即可，不需要add_field和remove_field，除非不想使用location这个名字把字段换成别的名字
            # 添加自己的坐标字段名称my_location
            # add_field => [ "[geoip][my_location]", "%{[geoip][longitude]}" ]
            # add_field => [ "[geoip][my_location]", "%{[geoip][latitude]}"  ]
            # 指定保留geoip的字段，注意geoip和geoip2字段的区别
            # fields => ["country_name", "country_code2","region_name", "city_name", "real_region_name", "latitude", "longitude"]
            # remove_field => [ "[geoip][longitude]", "[geoip][latitude]" ]
        }
        # 需要单独安装geoip2插件，不使用geoip是因为对IPv6支持的不好，logstash-plugin install logstash-filter-geoip2
        # 但是ES 2.x版本也只支持IPv4，不支持IPv6类型存储，所以最后将access.log中的IP统一都使用IPv4
#        geoip2 {
#            source => "client_ip"
#            target => "geoip"
#            # 这里指定好解压后GeoIP数据库文件的位置
#            database => "/usr/local/logstash/geoip/GeoLite2-City.mmdb"
#            # 指定保留geoip2的字段，注意geoip和geoip2字段的区别
#            # fields => ["city_name", "continent_code", "country_code2", "country_code3", "country_name", "dma_code", "ip", "postal_code", "region_name", "region_code", "timezone", "location"]
#        }
        mutate {
            # 使用自己的坐标字段名称my_location
            # convert => [ "[geoip][my_location]", "float" ]
            convert => [ "[geoip][location]", "float" ]
        }
        date {
            match => ["client_timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
            target => "@timestamp"
        }
    }
    if [type] == "node_proxy" {
        grok {
            match => {
                "message" => "%{MATCH_ALL:@message}"
            }
        }
        json {
            source => "@message"
        }
        date {
            match => ["timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
            target => "@timestamp"
            # 设置使用UTC标准时区，因为node的时间戳是UTC标准时区，不是中国时区，我们可以使用指定timezone来转换成long类型的@timestamp
            locale => "en"
            timezone => "+00:00"
        }
    }
}

output {
    stdout {
        codec => rubydebug
    }
    if [type] == "node_track" {
        elasticsearch {
            codec => "json"
            hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
            index => "node_track_logs_index_%{+YYYY.MM.dd}"
            document_type => "%{type}"
            template => "/usr/local/elasticsearch/template/node_track_logs_original_template.json"
            template_name => "node_track_logs_original_template"
            template_overwrite => true
            workers => 1
            flush_size => 20000
            idle_flush_time => 10
        }
    }
    if [type] == "node_access" {
        elasticsearch {
            codec => "json"
            hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
            index => "node_access_logs_index_%{+YYYY.MM.dd}"
            document_type => "%{type}"
            template => "/usr/local/elasticsearch/template/node_access_logs_original_template.json"
            template_name => "node_access_logs_original_template"
            template_overwrite => true
            workers => 1
            flush_size => 20000
            idle_flush_time => 10
        }
    }
    if [type] == "node_proxy" {
        elasticsearch {
            codec => "json"
            hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
            index => "node_proxy_logs_index_%{+YYYY.MM.dd}"
            document_type => "%{type}"
            template => "/usr/local/elasticsearch/template/node_proxy_logs_original_template.json"
            template_name => "node_proxy_logs_original_template"
            template_overwrite => true
            workers => 1
            flush_size => 20000
            idle_flush_time => 10
        }
    }
}

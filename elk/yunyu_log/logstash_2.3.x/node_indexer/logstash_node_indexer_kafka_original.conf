input {
    kafka {
        # 指定Zookeeper集群地址
        zk_connect => "hadoop1:2181,hadoop2:2181,hadoop3:2181"
        # 指定当前消费者的group_id，group_id不能和其他logstash消费者相同，否则同时启动多个Logstash消费者offset会被覆盖
        group_id => "logstash_node"
        # 指定消费的Topic
        topic_id => "node_log"
        # 指定消费的内容类型（默认是json）
        codec => "json"
        # 设置Consumer消费者从Kafka最开始的消息开始消费，必须结合"auto_offset_reset => smallest"一起使用
        reset_beginning => true
        # 设置如果Consumer消费者还没有创建offset或者offset非法，从最开始的消息开始消费还是从最新的消息开始消费
        auto_offset_reset => "smallest"
    }
}

filter {
    if [type] == "node_track" {
        grok {
            match => {
                "message" => "%{MATCH_ALL:@message}"
            }
        }
        json {
            source => "@message"
        }
        if "_jsonparsefailure" in [tags] {
            drop {}
        } else {
            if [logs] {
                # 将logs数组对象进行拆分
                split {
                    field => "logs"
                }
                date {
                    match => ["timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
                    target => "@timestamp"
                    # 设置使用UTC标准时区，因为node的时间戳是UTC标准时区，不是中国时区，我们可以使用指定timezone来转换成long类型的@timestamp
                    locale => "en"
                    timezone => "+00:00"
                }
            } else {
                drop {}
            }
        }
    }
#    if [type] == "node_access" {
#        grok {
#            match => {
#                "message" => "%{NODE_ACCESS_LOGS}"
#            }
#        }
#        if "_grokparsefailure" in [tags] {
#            drop { }
#        }
#    }
    if [type] == "node_proxy" {
        grok {
            match => {
                "message" => "%{MATCH_ALL:@message}"
            }
        }
        json {
            source => "@message"
        }
        date {
            match => ["timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
            target => "@timestamp"
            # 设置使用UTC标准时区，因为node的时间戳是UTC标准时区，不是中国时区，我们可以使用指定timezone来转换成long类型的@timestamp
            locale => "en"
            timezone => "+00:00"
        }
    }
}

output {
    stdout {
        codec => rubydebug
    }
    if [type] == "node_track" {
        elasticsearch {
            codec => "json"
            hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
            index => "node_track_logs_index_%{+YYYY.MM.dd}"
            document_type => "%{type}"
            template => "/usr/local/elasticsearch/template/node_track_logs_original_template.json"
            template_name => "node_track_logs_original_template"
            template_overwrite => true
            workers => 1
            flush_size => 20000
            idle_flush_time => 10
        }
    }
    if [type] == "node_access" {
        elasticsearch {
            codec => "json"
            hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
            index => "node_access_logs_index_%{+YYYY.MM.dd}"
            document_type => "%{type}"
            template => "/usr/local/elasticsearch/template/node_access_logs_original_template.json"
            template_name => "node_access_logs_original_template"
            template_overwrite => true
            workers => 1
            flush_size => 20000
            idle_flush_time => 10
        }
    }
    if [type] == "node_proxy" {
        elasticsearch {
            codec => "json"
            hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
            index => "node_proxy_logs_index_%{+YYYY.MM.dd}"
            document_type => "%{type}"
            template => "/usr/local/elasticsearch/template/node_proxy_logs_original_template.json"
            template_name => "node_proxy_logs_original_template"
            template_overwrite => true
            workers => 1
            flush_size => 20000
            idle_flush_time => 10
        }
    }
}

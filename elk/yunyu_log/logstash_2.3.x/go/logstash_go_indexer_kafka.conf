input {
    kafka {
        # 指定Zookeeper集群地址
        zk_connect => "test_zk:2181"
        # 指定当前消费者的group_id
        group_id => "logstash"
        # 指定消费的Topic
        topic_id => "golog"
        # 指定消费的内容类型（默认是json）
        codec => "plain"
        # 设置Consumer消费者从Kafka最开始的消息开始消费，必须结合"auto_offset_reset => smallest"一起使用
        reset_beginning => true
        # 设置如果Consumer消费者还没有创建offset或者offset非法，从最开始的消息开始消费还是从最新的消息开始消费
        auto_offset_reset => "smallest"
        type => "go"
    }
}

filter {
    grok {
        match => {
            "message" => "%{GO_LOGS}"
        }
        remove_field => ["sss", "message"]
    }
    date {
        match => ["timestamp", "MMdd HH:mm:ss.SSS"]
        target => "@timestamp"
    }
    if "_grokparsefailure" in [tags] {
        drop { }
    }
}

output {
    stdout {
        codec => rubydebug
    }
    elasticsearch {
        codec => "json"
        hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
        index => "go_logs_index"
        document_type => "%{type}"
        workers => 1
        flush_size => 20000
        idle_flush_time => 10
    }
}

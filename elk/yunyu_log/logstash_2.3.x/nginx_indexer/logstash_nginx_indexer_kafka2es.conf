input {
    kafka {
        # 指定Zookeeper集群地址
        zk_connect => "hadoop1:2181,hadoop2:2181,hadoop3:2181"
        # 指定当前消费者的group_id，group_id不能和其他logstash消费者相同，否则同时启动多个Logstash消费者offset会被覆盖
        group_id => "logstash_nginx"
        # 指定消费的Topic
        topic_id => "nginx_log"
        # 指定消费的内容类型（默认是json）
        codec => "json"
        # 设置Consumer消费者从Kafka最开始的消息开始消费，必须结合"auto_offset_reset => smallest"一起使用
        reset_beginning => true
        # 设置如果Consumer消费者还没有创建offset或者offset非法，从最开始的消息开始消费还是从最新的消息开始消费
        auto_offset_reset => "smallest"
    }
}

filter {
    if [type] == "nginx_access" {
        grok {
            match => {
                "message" => "%{NGINX_ACCESS_LOGS}"
            }
        }
        if "_grokparsefailure" in [tags] {
            drop { }
        }
        # 将client_ip为"-"的转换成"0.0.0.0"，或者直接删除client_ip字段，两种方式的效果都一样，如果IP地址是"-"就会删除client_ip字段，否则geoip转换会报错
        if [client_ip] {
            if [client_ip] == "-" {
                mutate {
                    replace => { "client_ip" => "0.0.0.0" }
                    # remove_field => [ "client_ip" ]
                }
            } else {
                split {
                    field => "client_ip"
                    terminator => ", "
                }
            }
        }
        # 这里是geoip指定字段的用法，不适用于geoip2
        geoip {
            source => "client_ip"
            target => "geoip"
            # 这里指定好解压后GeoIP数据库文件的位置
            database => "/usr/local/logstash/geoip/GeoLiteCity.dat"
            # 直接使用location字段即可，不需要add_field和remove_field，除非不想使用location这个名字把字段换成别的名字
            # 添加自己的坐标字段名称my_location
            # add_field => [ "[geoip][my_location]", "%{[geoip][longitude]}" ]
            # add_field => [ "[geoip][my_location]", "%{[geoip][latitude]}"  ]
            # 指定保留geoip的字段，注意geoip和geoip2字段的区别
            # fields => ["country_name", "country_code2","region_name", "city_name", "real_region_name", "latitude", "longitude"]
            # remove_field => [ "[geoip][longitude]", "[geoip][latitude]" ]
        }
        # 需要单独安装geoip2插件，不使用geoip是因为对IPv6支持的不好，logstash-plugin install logstash-filter-geoip2
        # 但是ES 2.x版本也只支持IPv4，不支持IPv6类型存储，所以最后将access.log中的IP统一都使用IPv4
#        geoip2 {
#            source => "client_ip"
#            target => "geoip"
#            # 这里指定好解压后GeoIP数据库文件的位置
#            database => "/usr/local/logstash/geoip/GeoLite2-City.mmdb"
#            # 指定保留geoip2的字段，注意geoip和geoip2字段的区别
#            # fields => ["city_name", "continent_code", "country_code2", "country_code3", "country_name", "dma_code", "ip", "postal_code", "region_name", "region_code", "timezone", "location"]
#        }
        mutate {
            # 使用自己的坐标字段名称my_location
            # convert => [ "[geoip][my_location]", "float" ]
            convert => [ "[geoip][location]", "float" ]

            # 将request_time和upstream_response_time转换成float，否则ES查询出来的是string类型
            convert => [ "[request_time]", "float" ]
            convert => [ "[upstream_response_time]", "float" ]
        }
        date {
            match => ["client_timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
            target => "@timestamp"
        }
    }
}

output {
    stdout {
        codec => rubydebug
    }
    if [type] == "nginx_access" {
        elasticsearch {
            codec => "json"
            hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
            index => "nginx_access_logs_index_%{+YYYY.MM.dd}"
            document_type => "%{type}"
            template => "/usr/local/elasticsearch/template/nginx_access_logs_template.json"
            template_name => "nginx_access_logs_template"
            template_overwrite => true
            workers => 1
            flush_size => 20000
            idle_flush_time => 10
        }
    }
}

input {
    kafka {
        # 指定Zookeeper集群地址
        zk_connect => "hadoop1:2181,hadoop2:2181,hadoop3:2181"
        # 指定当前消费者的group_id，group_id不能和其他logstash消费者相同，否则同时启动多个Logstash消费者offset会被覆盖
        group_id => "logstash_php"
        # 指定消费的Topic
        topic_id => "php_log"
        # 指定消费的内容类型（默认是json）
        codec => "json"
        # 设置Consumer消费者从Kafka最开始的消息开始消费，必须结合"auto_offset_reset => smallest"一起使用
        reset_beginning => true
        # 设置如果Consumer消费者还没有创建offset或者offset非法，从最开始的消息开始消费还是从最新的消息开始消费
        auto_offset_reset => "smallest"
    }
}

output {
    stdout {
        codec => rubydebug
    }
    webhdfs {
        workers => 1
        # hdfs的namenode地址
        host => "hadoop1"
        # Hadoop的webhdfs使用的端口
        port => 50070
        # hadoop运行的用户，以这个用户的权限去写入hdfs
        user => "yunyu"
        # 按年月日建目录，按type和小时建log文件
        path => "/logstash/%{+YYYY}/%{+MM}/%{+dd}/php-%{+HH}.log"
        flush_size => 5000
        # 压缩格式，可以不压缩
        # compression => "snappy"
        idle_flush_time => 5
        retry_interval => 5
    }
}

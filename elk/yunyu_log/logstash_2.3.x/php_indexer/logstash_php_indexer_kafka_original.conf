input {
    kafka {
        # 指定Zookeeper集群地址
        zk_connect => "hadoop1:2181,hadoop2:2181,hadoop3:2181"
        # 指定当前消费者的group_id，group_id不能和其他logstash消费者相同，否则同时启动多个Logstash消费者offset会被覆盖
        group_id => "logstash_php"
        # 指定消费的Topic
        topic_id => "php_log"
        # 指定消费的内容类型（默认是json）
        codec => "json"
        # 设置Consumer消费者从Kafka最开始的消息开始消费，必须结合"auto_offset_reset => smallest"一起使用
        reset_beginning => true
        # 设置如果Consumer消费者还没有创建offset或者offset非法，从最开始的消息开始消费还是从最新的消息开始消费
        auto_offset_reset => "smallest"
    }
}

filter {
    grok {
        match => {
            "message" => "%{PHP_LOGS}"
        }
    }
    mutate {
        replace => {
            "type" => "php_%{type}"
        }
        rename => ["message", "@message"]
    }
    date {
        match => ["timestamp", "YYYY-MM-dd HH:mm:ss"]
        target => "@timestamp"
    }
    if "_grokparsefailure" in [tags] {
        drop { }
    }
}

output {
    stdout {
        codec => rubydebug
    }
    elasticsearch {
        codec => "json"
        hosts => ["hadoop1:9200", "hadoop2:9200", "hadoop3:9200"]
        index => "php_logs_index_%{+YYYY.MM.dd}"
        document_type => "%{type}"
        template => "/usr/local/elasticsearch/template/php_logs_original_template.json"
        template_name => "php_logs_original_template"
        template_overwrite => true
        workers => 1
        flush_size => 20000
        idle_flush_time => 10
    }
}
